{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ15p9LvvSOr",
        "outputId": "be9678b1-f845-4c8d-be6b-4b88f471920e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "300/300 [==============================] - 35s 112ms/step - loss: 0.3869 - accuracy: 0.8808 - val_loss: 0.0855 - val_accuracy: 0.9721\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 30s 100ms/step - loss: 0.1003 - accuracy: 0.9690 - val_loss: 0.0470 - val_accuracy: 0.9846\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 30s 102ms/step - loss: 0.0711 - accuracy: 0.9778 - val_loss: 0.0407 - val_accuracy: 0.9861\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 30s 101ms/step - loss: 0.0563 - accuracy: 0.9829 - val_loss: 0.0383 - val_accuracy: 0.9877\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 31s 104ms/step - loss: 0.0501 - accuracy: 0.9843 - val_loss: 0.0343 - val_accuracy: 0.9892\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 30s 101ms/step - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0293 - val_accuracy: 0.9904\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 30s 100ms/step - loss: 0.0377 - accuracy: 0.9880 - val_loss: 0.0302 - val_accuracy: 0.9904\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 30s 100ms/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0285 - val_accuracy: 0.9899\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 30s 100ms/step - loss: 0.0321 - accuracy: 0.9900 - val_loss: 0.0288 - val_accuracy: 0.9906\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 30s 101ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0288 - val_accuracy: 0.9906\n",
            "Large CNN Error: 0.94%\n"
          ]
        }
      ],
      "source": [
        "# Larger CNN for the MNIST Dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define the larger model\n",
        "def larger_model():\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        " model.add(MaxPooling2D())\n",
        " model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        " model.add(MaxPooling2D())\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(50, activation='relu'))\n",
        " model.add(Dense(num_classes, activation='softmax'))\n",
        " # Compile model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "# build the model\n",
        "model = larger_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFgch6DMvWzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}